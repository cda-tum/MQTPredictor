{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest version of the mqt-predictor\n",
    "!cd .. && cd .. && uv pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import qiskit.qasm2\n",
    "import scipy as sp\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import KFold, ParameterGrid, train_test_split\n",
    "\n",
    "from mqt.bench.devices import get_device_by_name\n",
    "from mqt.predictor.reg import hellinger_distance\n",
    "from mqt.predictor.reward import (\n",
    "    calc_device_specific_features,\n",
    "    estimated_success_probability,\n",
    "    expected_fidelity,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT AND PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to the current data.zip file\n",
    "data_dir = Path.cwd() / \"data\"\n",
    "\n",
    "# Ensure the zip file exists\n",
    "zip_file_path = Path.cwd() / \"data.zip\"\n",
    "if not Path.exists(zip_file_path):\n",
    "    msg = f\"Zip file not found at {zip_file_path}\"\n",
    "    raise FileNotFoundError(msg)\n",
    "\n",
    "# Unzip the data files into the current directory\n",
    "with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(data_dir)  # Extracts into the current directory\n",
    "\n",
    "print(f\"Files extracted from {zip_file_path} into the current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries to store the calculated figures of merit and Hellinger distance\n",
    "depth, esp, fidelity, num_gates, hellinger = {}, {}, {}, {}, {}\n",
    "\n",
    "qpus = [\"apollo\", \"qexa\"]\n",
    "for qpu in qpus:\n",
    "    depth[qpu], esp[qpu], fidelity[qpu], num_gates[qpu], hellinger[qpu] = {}, {}, {}, {}, {}\n",
    "    circuits = []  # TODO: only for debugging\n",
    "\n",
    "    try:  # Import QPU\n",
    "        device = get_device_by_name(\"iqm_\" + qpu)\n",
    "    except Exception:\n",
    "        # Device 'iqm_qexa' calibration data not yet \"publicly\" available\n",
    "        print(f\"Device 'iqm_{qpu}' not found among available providers.\")\n",
    "        print(\"Using precomputed ESP and fidelity values with historical calibration data.\")\n",
    "        device = None\n",
    "\n",
    "    # Get a list of all execution files\n",
    "    execution_files = sorted((data_dir / \"execution\" / qpu).glob(\"*.txt\"))\n",
    "    print(f\"Found {len(execution_files)} execution files for {qpu}\")\n",
    "\n",
    "    # Iterate over the execution files\n",
    "    for execution_file in execution_files:\n",
    "        # file_name is sth. like 'wstate_indep_qiskit_20'\n",
    "        file_name = execution_file.stem\n",
    "        try:\n",
    "            # Load the corresponding circuit\n",
    "            circuit = qiskit.qasm2.load(data_dir / \"qasm\" / f\"{file_name}.qasm\")\n",
    "\n",
    "            # Load the noiseless simulation file\n",
    "            noiseless_file = data_dir / \"simulation\" / qpu / f\"{file_name}.txt\"\n",
    "\n",
    "            # Open the files and read the counts\n",
    "            with Path.open(noiseless_file, encoding=\"utf-8\") as f:\n",
    "                noiseless_counts = ast.literal_eval(f.read())\n",
    "            with Path.open(execution_file, encoding=\"utf-8\") as f:\n",
    "                execution_counts = ast.literal_eval(f.read())\n",
    "\n",
    "            num_execution_shots = sum(execution_counts.values())\n",
    "            num_noiseless_shots = sum(noiseless_counts.values())\n",
    "\n",
    "            # Transform counts to probabilities\n",
    "            noiseless_probs_org = {k: v / num_noiseless_shots for k, v in noiseless_counts.items()}\n",
    "\n",
    "            # Only keep the states that are distinguishable with the number of execution shots\n",
    "            noiseless_probs = {k: v for k, v in noiseless_probs_org.items() if v * num_execution_shots >= 1}\n",
    "\n",
    "            # Get the states and their noiseless probabilities\n",
    "            states = sorted(set(noiseless_counts.keys()).union(execution_counts.keys()))\n",
    "            noiseless_probs_all = np.array([noiseless_probs.get(state, 0) for state in states])\n",
    "\n",
    "            # Get the execution probabilities\n",
    "            execution_counts_all = np.array([execution_counts.get(state, 0) for state in states])\n",
    "            execution_probs_all = execution_counts_all / num_execution_shots\n",
    "\n",
    "            # If they do not resemble a probability distribution, skip the file (e.g., due to too few shots)\n",
    "            if not np.isclose(sum(noiseless_probs_all), 1, 0.05) or not np.isclose(sum(execution_probs_all), 1, 0.05):\n",
    "                # print(f\"Skipping {file_name} because probabilities do not sum to 1\")\n",
    "                continue\n",
    "\n",
    "            # File paths for the figures of merit\n",
    "            path = data_dir / \"foms\" / qpu / file_name\n",
    "\n",
    "            num_gates_file = path.with_name(f\"{path.name}_num_gates.txt\")\n",
    "            depth_file = path.with_name(f\"{path.name}_depth.txt\")\n",
    "            fidelity_file = path.with_name(f\"{path.name}_fidelity.txt\")\n",
    "            esp_file = path.with_name(f\"{path.name}_esp.txt\")\n",
    "\n",
    "            # Hellinger distance will be saved as labels\n",
    "            labels_file = data_dir / \"labels\" / qpu / file_name\n",
    "            hellinger_file = labels_file.with_name(f\"{labels_file.name}_hellinger.txt\")\n",
    "\n",
    "            # Calculate all values and save them to a file or load them from the file if it exists\n",
    "\n",
    "            # Depth\n",
    "            if not depth_file.exists():\n",
    "                d = circuit.depth()\n",
    "                with Path.open(depth_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(str(d))\n",
    "            else:\n",
    "                with Path.open(depth_file, encoding=\"utf-8\") as f:\n",
    "                    d = int(f.read())\n",
    "            if d > 1000:\n",
    "                # print(f\"Skipping {file_name} because depth is too high\")\n",
    "                continue\n",
    "            depth[qpu][file_name] = d\n",
    "\n",
    "            # Fidelity\n",
    "            if device and not fidelity_file.exists():\n",
    "                fidelity[qpu][file_name] = expected_fidelity(circuit, device)\n",
    "                with Path.open(fidelity_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(str(fidelity[qpu][file_name]))\n",
    "            else:\n",
    "                with Path.open(fidelity_file, encoding=\"utf-8\") as f:\n",
    "                    fidelity[qpu][file_name] = float(f.read())\n",
    "\n",
    "            # ESP\n",
    "            if device and not esp_file.exists():\n",
    "                esp[qpu][file_name] = estimated_success_probability(circuit, device)\n",
    "                with Path.open(esp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(str(esp[qpu][file_name]))\n",
    "            else:\n",
    "                with Path.open(esp_file, encoding=\"utf-8\") as f:\n",
    "                    esp[qpu][file_name] = float(f.read())\n",
    "\n",
    "            # Number of gates\n",
    "            if not num_gates_file.exists():\n",
    "                num_gates[qpu][file_name] = sum(circuit.count_ops().values())\n",
    "                with Path.open(num_gates_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(str(num_gates[qpu][file_name]))\n",
    "            else:\n",
    "                with Path.open(num_gates_file, encoding=\"utf-8\") as f:\n",
    "                    num_gates[qpu][file_name] = int(f.read())\n",
    "\n",
    "            # Execution vs noiseless Hellinger distance\n",
    "            if not hellinger_file.exists():\n",
    "                hellinger[qpu][file_name] = hellinger_distance(noiseless_probs_all, execution_probs_all)\n",
    "                with Path.open(hellinger_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(str(hellinger[qpu][file_name]))\n",
    "            else:\n",
    "                with Path.open(hellinger_file, encoding=\"utf-8\") as f:\n",
    "                    hellinger[qpu][file_name] = float(f.read())\n",
    "\n",
    "            circuits.append(circuit)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {qpu}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\n",
    "        f\"Stored {len(depth[qpu])} files for {qpu}, after removing too high depths and non-probability distributions. \\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION OF CURRENT FIGURES OF MERIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"#FE6100\", \"#648FFF\"]\n",
    "names = [\"Q20-A\", \"Q20-B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(9, 9))\n",
    "data = {}\n",
    "\n",
    "for i, qpu in enumerate(qpus):\n",
    "    # Collect data for plot\n",
    "    depth_list = list(depth[qpu].values())\n",
    "    esp_list = list(esp[qpu].values())\n",
    "    fidelity_list = list(fidelity[qpu].values())\n",
    "    num_gates_list = list(num_gates[qpu].values())\n",
    "    execution_noiseless_dist_list = list(hellinger[qpu].values())\n",
    "\n",
    "    data[qpu] = [\n",
    "        (num_gates_list, execution_noiseless_dist_list, \"Gate Count\", \"Hellinger Distance\"),\n",
    "        (depth_list, execution_noiseless_dist_list, \"Circuit Depth\", \"Hellinger Distance\"),\n",
    "        (esp_list, execution_noiseless_dist_list, \"Estimated Success Probability\", \"Hellinger Distance\"),\n",
    "        (fidelity_list, execution_noiseless_dist_list, \"Expected Fidelity\", \"Hellinger Distance\"),\n",
    "    ]\n",
    "\n",
    "    for ax, (x, y, xlabel, ylabel) in zip(axs.flatten(), data[qpu], strict=False):\n",
    "        # Normalize for Pearson correlation\n",
    "        x_max, y_max = np.max(x), np.max(y)\n",
    "        x_norm, y_norm = np.array(x) / x_max, np.array(y) / y_max\n",
    "\n",
    "        # Calculate Pearson correlation\n",
    "        slope, intercept, rval, pval, std_err = sp.stats.linregress(x_norm, y_norm)\n",
    "        x_plot = np.linspace(np.min(x) - 1, np.max(x) + 1, 100)\n",
    "        y_plot = (slope * (x_plot / x_max) + intercept) * y_max\n",
    "        ax.plot(x_plot, y_plot, color=cols[i], linewidth=0.5, alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "        # Should be the same as the calculated Pearson correlation\n",
    "        r, p = pearsonr(x_norm, y_norm)\n",
    "        assert np.isclose(r, rval), f\"r: {r}, rval: {rval}\"\n",
    "        assert np.isclose(p, pval), f\"p: {p}, pval: {pval}\"\n",
    "\n",
    "        # Plot the data\n",
    "        pearson_label = names[i] + f\": Pearson $r = {r:.2f}$\"\n",
    "        ax.scatter(x, y, color=cols[i], marker=\".\", label=pearson_label, alpha=0.5)\n",
    "\n",
    "        # Adjust x-axis\n",
    "        if xlabel == \"Gate Count\":\n",
    "            ax.set_xlim(0, 2000)\n",
    "            ax.xaxis.set_major_locator(MultipleLocator(500))\n",
    "        elif xlabel == \"Circuit Depth\":\n",
    "            ax.set_xlim(0, 1000)\n",
    "            ax.xaxis.set_major_locator(MultipleLocator(250))\n",
    "        else:\n",
    "            ax.set_xlim(0, 1)\n",
    "        ax.set_xlabel(xlabel, labelpad=1)\n",
    "\n",
    "        # Adjust y-axis\n",
    "        ax.set_ylim(0.0, 1.0)\n",
    "        ax.set_ylabel(ylabel, labelpad=1)\n",
    "        ax.yaxis.set_major_locator(MultipleLocator(0.2))\n",
    "\n",
    "        # Adjust tick size and tick spacing\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", width=0.25)\n",
    "\n",
    "        # Set border line width\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(0.25)\n",
    "\n",
    "        ax.legend(scatterpoints=1)  # Adjust legend text size\n",
    "        legend = ax.legend(markerscale=2.0)\n",
    "\n",
    "        print(qpu, xlabel, \"correlation\", np.round(r, 3))\n",
    "\n",
    "fig.tight_layout(pad=0.1, h_pad=0.2, w_pad=0)\n",
    "plt.savefig(data_dir / \"figures/correlation.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROPOSED APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {qpu: [] for qpu in qpus}\n",
    "Y = {qpu: [] for qpu in qpus}\n",
    "\n",
    "# Both QPUs have the same architecture (only different calibration data)\n",
    "device = get_device_by_name(\"iqm_apollo\")\n",
    "\n",
    "for qpu in qpus:\n",
    "    # save the training data to\n",
    "    path = data_dir / \"features\" / qpu\n",
    "\n",
    "    native_gates = [\"r\", \"cz\"]  # iqm gates\n",
    "\n",
    "    # Get a list of all execution files\n",
    "    qasm_files = sorted((data_dir / \"execution\" / qpu).glob(\"*.txt\"))\n",
    "\n",
    "    # Iterate over the execution files\n",
    "    for qasm_file in qasm_files:\n",
    "        # file_name is sth. like 'qft_10'\n",
    "        file_name = qasm_file.stem\n",
    "\n",
    "        try:\n",
    "            circ = qiskit.qasm2.load(data_dir / \"qasm\" / f\"{file_name}.qasm\")\n",
    "            try:\n",
    "                y = hellinger[qpu][file_name]\n",
    "                Y[qpu].append(y)\n",
    "            except KeyError:\n",
    "                continue  # No Hellinger distance available, e.g. if depth was too high\n",
    "\n",
    "            file = path / f\"{file_name}.txt\"\n",
    "            if not file.exists():\n",
    "                x_dict = calc_device_specific_features(circ, device)\n",
    "                x = list(x_dict.values())\n",
    "                X[qpu].append(x)\n",
    "\n",
    "                np.savetxt(file, np.array(x))\n",
    "                # print(f\"Saved features for {file_name}\")\n",
    "            else:\n",
    "                X[qpu].append(np.loadtxt(file))\n",
    "                # print(f\"Loaded features for {file_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing circuit {qasm_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Extracted features and labels for {len(X[qpu])} files for {qpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate which features to use for the prediction\n",
    "non_zero_vector = {qpu: {} for qpu in qpus}\n",
    "\n",
    "for qpu in qpus:\n",
    "    gate_dict = {\"R Gate Count\": True, \"CZ Gate Count\": True}\n",
    "    num_qubits = 20  # Same for both IQM QPUs\n",
    "    qubit_dict = {f\"Qubit{i}\": True for i in range(num_qubits)}\n",
    "\n",
    "    supermarq_plus_dict = {\n",
    "        \"Circuit Depth\": True,\n",
    "        \"Number of Qubits\": False,\n",
    "        \"Critical Depth\": True,\n",
    "        \"Entanglement Ratio\": True,\n",
    "        \"Parallelism\": True,\n",
    "        \"Liveness\": True,\n",
    "        \"Directed Program Comm.\": True,\n",
    "        \"Single-Qubit Gate Ratio\": True,\n",
    "        \"Multi-Qubit Gate Ratio\": True,\n",
    "    }\n",
    "\n",
    "    non_zero_vector[qpu] = {**gate_dict, **qubit_dict, **supermarq_plus_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = {}, {}, {}, {}\n",
    "\n",
    "for qpu in qpus:\n",
    "    X_arr = np.array(X[qpu])\n",
    "    Y_arr = np.array(Y[qpu])\n",
    "\n",
    "    # Mask the features\n",
    "    mask = np.array(list(non_zero_vector[qpu].values()))\n",
    "    X_masked = X_arr[:, mask]\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train[qpu], X_test[qpu], y_train[qpu], y_test[qpu] = train_test_split(\n",
    "        X_masked, Y_arr, test_size=0.2, random_state=123\n",
    "    )\n",
    "    print(f\"Training data shape for {qpu}: features {X_train[qpu].shape}, labels {y_train[qpu].shape}\")\n",
    "    print(f\"Test data shape for {qpu}: featuers {X_test[qpu].shape}, labels {y_test[qpu].shape} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO BE DELETED! ONLY FOR TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device_by_name(\"iqm_apollo\")\n",
    "quantum_circuits = circuits\n",
    "\n",
    "# prepare the distribution\n",
    "rng = np.random.Generator(np.random.PCG64(123))\n",
    "noisy_distribution = [rng.random() for _ in range(len(circuits))]\n",
    "noiseless_distribution = [rng.random() for _ in range(len(circuits))]\n",
    "# normalize\n",
    "noisy_distribution = noisy_distribution / np.sum(noisy_distribution)\n",
    "noiseless_distribution = noiseless_distribution / np.sum(noiseless_distribution)\n",
    "\n",
    "noisy_distributions = [noisy_distribution for _ in range(len(circuits))]\n",
    "noiseless_distributions = [noiseless_distribution for _ in range(len(circuits))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mqt.predictor.reward import calc_device_specific_features\n",
    "\n",
    "feature_vector_list = []\n",
    "for qc in quantum_circuits:\n",
    "    feature_dict = calc_device_specific_features(qc, device)\n",
    "    feature_vector = np.array(list(feature_dict.values()))\n",
    "    feature_vector_list.append(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mqt.predictor.ml import hellinger_distance\n",
    "\n",
    "labels_list = []\n",
    "for noisy, noiseless in zip(noisy_distributions, noiseless_distributions, strict=False):\n",
    "    distance_label = hellinger_distance(noisy, noiseless)\n",
    "    labels_list.append(distance_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mqt.predictor.ml import Predictor\n",
    "from mqt.predictor.reward import estimated_hellinger_distance\n",
    "\n",
    "training_data = [(feat, label) for feat, label in zip(feature_vector_list, labels_list, strict=False)]\n",
    "\n",
    "pred = Predictor(figure_of_merit=\"hellinger_distance\")\n",
    "pred.save_training_data(training_data)\n",
    "\n",
    "pred.train_random_forest_model(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimated_hellinger_distance(circuits[0], device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# Save best parameter index to file\n",
    "def save_best_params(idx: int, best_params: dict, path: str) -> None:\n",
    "    file = {idx: best_params}\n",
    "    with Path.open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(file))\n",
    "\n",
    "\n",
    "# Load best parameter index from file\n",
    "def load_best_params(path: str) -> tuple[int, dict]:\n",
    "    with Path.open(path, encoding=\"utf-8\") as f:\n",
    "        file = eval(f.read())\n",
    "    idx, best_params = next(iter(file.keys())), next(iter(file.values()))\n",
    "    return idx, best_params\n",
    "\n",
    "\n",
    "seed = 2\n",
    "gen = np.random.default_rng(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"RandomForestRegressor\": {\n",
    "        \"model\": RandomForestRegressor(random_state=seed, verbose=0, n_jobs=5),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [30, 40, 50, 60],  # Number of trees in the forest; more trees reduce variance.\n",
    "            \"criterion\": [\"absolute_error\", \"squared_error\"],  # Metric to evaluate splits; affects tree quality.\n",
    "            \"max_depth\": [\n",
    "                10,\n",
    "                20,\n",
    "                30,\n",
    "                None,\n",
    "            ],  # Max depth of trees; smaller values prevent overly complex trees (reduce overfitting).\n",
    "            \"min_samples_split\": [\n",
    "                4,\n",
    "                5,\n",
    "                0.1,\n",
    "            ],  # Min samples to split an internal node; higher values reduce sensitivity to noise.\n",
    "            \"min_samples_leaf\": [\n",
    "                2,\n",
    "                3,\n",
    "                4,\n",
    "                5,\n",
    "                0.1,\n",
    "            ],  # Min samples in a leaf node; larger values create simpler, generalized trees.\n",
    "            # This is the main RANDOM FOREST parameter\n",
    "            \"max_features\": [\n",
    "                \"sqrt\",\n",
    "                \"log2\",\n",
    "                None,\n",
    "            ],  # Max features to consider at each split; lower values reduce overfitting by randomness.\n",
    "            \"max_leaf_nodes\": [\n",
    "                None,\n",
    "                20,\n",
    "                40,\n",
    "                60,\n",
    "                80,\n",
    "                100,\n",
    "            ],  # Max leaf nodes in the tree; fewer nodes limit model complexity.\n",
    "            \"min_impurity_decrease\": [\n",
    "                0.01,\n",
    "                0.1,\n",
    "                0.2,\n",
    "            ],  # Min impurity decrease for a split; higher values prevent splits on minor gains.\n",
    "            \"ccp_alpha\": [\n",
    "                0.01,\n",
    "                0.05,\n",
    "            ],  # Complexity parameter for pruning; larger values simplify the tree (reduce overfitting).\n",
    "            \"max_samples\": [\n",
    "                0.5,\n",
    "                1.0,\n",
    "            ],  # Max samples to draw for training each tree; smaller values increase diversity, reducing overfitting.\n",
    "            \"monotonic_cst\": [\n",
    "                [  # Monotonic constraints; improves interpretability, not directly related to overfitting.\n",
    "                    # -1 means decreasing, 0 means no constraint, and 1 means increasing\n",
    "                    # Gate counts\n",
    "                    1,  # R gate count\n",
    "                    1,  # CZ gate count\n",
    "                    # Qubit counts\n",
    "                    0,  # Qubit 0\n",
    "                    0,  # Qubit 1\n",
    "                    0,  # Qubit 2\n",
    "                    0,  # Qubit 3\n",
    "                    0,  # Qubit 4\n",
    "                    0,  # Qubit 5\n",
    "                    0,  # Qubit 6\n",
    "                    0,  # Qubit 7\n",
    "                    0,  # Qubit 8\n",
    "                    0,  # Qubit 9\n",
    "                    0,  # Qubit 10\n",
    "                    0,  # Qubit 11\n",
    "                    0,  # Qubit 12\n",
    "                    0,  # Qubit 13\n",
    "                    0,  # Qubit 14\n",
    "                    0,  # Qubit 15\n",
    "                    0,  # Qubit 16\n",
    "                    0,  # Qubit 17\n",
    "                    0,  # Qubit 18\n",
    "                    0,  # Qubit 19\n",
    "                    # Supermarq+\n",
    "                    1,  # Circuit depth\n",
    "                    0,  # Critical depth\n",
    "                    1,  # Entanglement ratio\n",
    "                    0,  # Parallelism\n",
    "                    0,  # Liveness\n",
    "                    0,  # Directed program communication\n",
    "                    0,  # Single qubit gates ratio\n",
    "                    0,  # Multi qubit gates ratio\n",
    "                ],\n",
    "                None,\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Grid search / training takes time!\n",
    "If training is disabled, the best parameters will be loaded from previous search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_training = False\n",
    "\n",
    "if enable_training:\n",
    "    # Initialize dictionary to store the best model for each QPU\n",
    "    best_model = dict.fromkeys(qpus)\n",
    "\n",
    "    # Path to save the best parameters\n",
    "    trainnig_path = data_dir / \"training\"\n",
    "\n",
    "    for qpu in qpus:\n",
    "        best_score = -np.inf\n",
    "\n",
    "        # Load the best parameters if they exist\n",
    "        filename = f\"{trainnig_path}/{qpu}_best_params.txt\"\n",
    "        if Path.exists(filename):\n",
    "            idx, best_params = load_best_params(filename)\n",
    "            print(f\"Loaded best parameters for {qpu}: {best_params}\")\n",
    "            continue\n",
    "        else:\n",
    "            idx = 0\n",
    "            best_params = {}\n",
    "            print(\"No best parameters found\")\n",
    "\n",
    "        for model_info in models.values():\n",
    "            param_grid = ParameterGrid(model_info[\"params\"])\n",
    "            gird = list(param_grid)\n",
    "            len_grid = len(gird)\n",
    "\n",
    "            for i, params in enumerate(gird[idx:]):\n",
    "                j = i + idx\n",
    "                print(f\"Progress: {j}/{len_grid}\")\n",
    "                model = model_info[\"model\"].set_params(**params)\n",
    "\n",
    "                # Initialize cross-validation\n",
    "                kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "                cv_scores = []\n",
    "\n",
    "                # Perform cross-validation\n",
    "                for train_index, val_index in kf.split(X_train[qpu]):\n",
    "                    X_train_split, X_val_split = X_train[qpu][train_index], X_train[qpu][val_index]\n",
    "                    y_train_split, y_val_split = y_train[qpu][train_index], y_train[qpu][val_index]\n",
    "\n",
    "                    # Train and validate\n",
    "                    model.fit(X_train_split, y_train_split)\n",
    "                    predictions = model.predict(X_val_split)\n",
    "                    score = pearsonr(predictions, y_val_split)[0]  # Pearson correlation coefficient\n",
    "                    cv_scores.append(score)\n",
    "\n",
    "                # Compute average score across all folds\n",
    "                mean_score = np.mean(cv_scores)\n",
    "\n",
    "                # Update the best model with a deep copy\n",
    "                if mean_score >= best_score:\n",
    "                    best_model[qpu] = copy.deepcopy(model)  # Ensure an independent copy is stored\n",
    "                    best_score = mean_score\n",
    "\n",
    "                    print(f\"Parameters: {params}\")\n",
    "                    print(f\"Cross-Validation Mean Score: {mean_score}\")\n",
    "                    print(f\"Associated Test Score: {pearsonr(best_model[qpu].predict(X_test[qpu]), y_test[qpu])[0]}\")\n",
    "\n",
    "                    # Save the best parameters to a file\n",
    "                    save_best_params(i, params, filename)\n",
    "\n",
    "        print(f\"Best overall model on {qpu}: {best_model[qpu]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best parameters\n",
    "training_path = data_dir / \"training\"\n",
    "best_model = dict.fromkeys(qpus)\n",
    "\n",
    "for qpu in qpus:\n",
    "    # Load the best parameters if they exist\n",
    "    filename = training_path / f\"{qpu}_best_params.txt\"\n",
    "    if Path.exists(filename):\n",
    "        idx, best_params = load_best_params(filename)\n",
    "        best_model[qpu] = RandomForestRegressor(random_state=seed, verbose=0, n_jobs=5).set_params(**best_params)\n",
    "        print(f\"Loaded best parameters for {qpu}: {best_params}\")\n",
    "    else:\n",
    "        print(\"No best parameters found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = data_dir.parent / \"trained_models\"\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "trained_models = {}\n",
    "train_model = True\n",
    "\n",
    "if train_model:\n",
    "    for qpu in qpus:\n",
    "        trained_models[qpu] = best_model[qpu].fit(X_train[qpu], y_train[qpu])\n",
    "\n",
    "        # save model to file\n",
    "        if best_model[qpu] is not None:\n",
    "            with Path.open(model_path / f\"iqm_{qpu}_rf_regressor.pkl\", \"wb\") as f:\n",
    "                pickle.dump(best_model[qpu], f)\n",
    "            with Path.open(model_path / f\"iqm_{qpu}_non_zero_indices.pkl\", \"wb\") as f:\n",
    "                pickle.dump(non_zero_vector[qpu], f)\n",
    "        print(f\"Saved model and index_dict to {model_path}\")\n",
    "\n",
    "else:  # load model from file\n",
    "    trained_models = {}\n",
    "    for qpu in qpus:\n",
    "        with Path.open(model_path / f\"iqm_{qpu}_rf_regressor.pkl\", \"rb\") as f:\n",
    "            trained_models[qpu] = pickle.load(f)\n",
    "        with Path.open(model_path / f\"iqm_{qpu}_non_zero_indices.pkl\", \"rb\") as f:\n",
    "            non_zero_vector[qpu] = pickle.load(f)\n",
    "        print(f\"Loaded model and index_dict from {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "data = {}\n",
    "for i, qpu in enumerate(qpus):\n",
    "    # Collect data for plot\n",
    "    model_prediction = list(trained_models[qpu].predict(X_test[qpu]))\n",
    "    true_labels = list(y_test[qpu])\n",
    "\n",
    "    ax = axs\n",
    "    x = model_prediction\n",
    "    y = true_labels\n",
    "    xlabel = \"Figure of Merit\"\n",
    "    ylabel = \"Hellinger distance\"\n",
    "\n",
    "    # Normalize for Pearson correlation\n",
    "    x_max, y_max = np.max(x), np.max(y)\n",
    "    x_norm, y_norm = np.array(x) / x_max, np.array(y) / y_max\n",
    "\n",
    "    # Calculate Pearson correlation\n",
    "    slope, intercept, rval, pval, std_err = sp.stats.linregress(x_norm, y_norm)\n",
    "    x_plot = np.linspace(np.min(x) - 1, np.max(x) + 1, 100)\n",
    "    y_plot = (slope * (x_plot / x_max) + intercept) * y_max\n",
    "    ax.plot(x_plot, y_plot, color=cols[i], linewidth=0.5, alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "    # Should be the same as the calculated Pearson correlation\n",
    "    r, p = pearsonr(x_norm, y_norm)\n",
    "    assert np.isclose(r, rval), f\"r: {r}, rval: {rval}\"\n",
    "    assert np.isclose(p, pval), f\"p: {p}, pval: {pval}\"\n",
    "\n",
    "    print(f\"{qpu}:  {xlabel}:\")\n",
    "    print(f\"Pearson r: {rval:.3f}, pval: {pval:.3f}\\n\")\n",
    "\n",
    "    pearson_label = names[i] + f\": Pearson $r = {r:.3f}$\"\n",
    "\n",
    "    # Plot the data\n",
    "    ax.scatter(x, y, color=cols[i], marker=\".\", label=pearson_label, alpha=0.7)\n",
    "\n",
    "    # Adjust x-axis\n",
    "    if xlabel == \"Number of gates\":\n",
    "        ax.set_xlim(0, 2000)\n",
    "        ax.xaxis.set_major_locator(MultipleLocator(500))\n",
    "    elif xlabel == \"Circuit depth\":\n",
    "        ax.set_xlim(0, 1000)\n",
    "        ax.xaxis.set_major_locator(MultipleLocator(250))\n",
    "    else:\n",
    "        ax.set_xlim(0, 1)\n",
    "    ax.set_xlabel(xlabel, labelpad=1)\n",
    "\n",
    "    # Adjust y-axis\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.set_ylabel(ylabel, labelpad=1)\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(0.2))\n",
    "\n",
    "    # Adjust tick size and tick spacing\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", width=0.25)\n",
    "\n",
    "    # Set border line width\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(0.25)\n",
    "\n",
    "    ax.legend(scatterpoints=1)  # Adjust legend text size\n",
    "\n",
    "    legend = ax.legend(markerscale=2.0)\n",
    "\n",
    "plt.savefig(data_dir / \"figures\" / \" prediction.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_qubit_features(imps: list, names: list) -> tuple[np.ndarray, list]:\n",
    "    \"\"\"Groups qubit features and returns the grouped importances.\"\"\"\n",
    "    qubit_indices = [i for i, name in enumerate(names) if name.startswith(\"Qubit\")]\n",
    "    qubit_importances = imps[qubit_indices]\n",
    "    qubit_importances_avg = np.average(qubit_importances, axis=0)\n",
    "\n",
    "    # Remove individual qubit importances\n",
    "    new_imps = [imps[i] for i in range(len(imps)) if i not in qubit_indices]\n",
    "    new_names = [name for i, name in enumerate(names) if i not in qubit_indices]\n",
    "\n",
    "    # Append the grouped qubit importances\n",
    "    new_imps.append(qubit_importances_avg)\n",
    "    new_names.append(\"Average qubit features\")\n",
    "\n",
    "    return np.array(new_imps), new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries for feature importances and permutation importances\n",
    "feature_importances = {}\n",
    "perm_importances = {}\n",
    "\n",
    "# Extract feature names (assuming they are the same for all QPUs)\n",
    "feature_names = [name for name in non_zero_vector[qpus[0]] if non_zero_vector[qpus[0]][name]]\n",
    "\n",
    "# Compute feature importance and permutation importance for each QPU\n",
    "for qpu in qpus:\n",
    "    feature_importances[qpu] = trained_models[qpu].feature_importances_\n",
    "\n",
    "    perm_importances[qpu] = permutation_importance(\n",
    "        trained_models[qpu], X_test[qpu], y_test[qpu], n_repeats=10, random_state=42\n",
    "    )[\"importances_mean\"]\n",
    "\n",
    "# Group qubit features\n",
    "grouped_feature_importances = {}\n",
    "grouped_perm_importances = {}\n",
    "\n",
    "# Also store the feature names after grouping\n",
    "grouped_feature_names = None\n",
    "grouped_perm_feature_names = None\n",
    "\n",
    "for qpu in qpus:\n",
    "    grouped_feature_importances[qpu], grouped_feature_names = group_qubit_features(\n",
    "        feature_importances[qpu], feature_names\n",
    "    )\n",
    "    grouped_perm_importances[qpu], grouped_perm_feature_names = group_qubit_features(\n",
    "        perm_importances[qpu], feature_names\n",
    "    )\n",
    "\n",
    "# Ensure all QPUs have the same feature ordering after grouping\n",
    "if grouped_feature_names != grouped_perm_feature_names:\n",
    "    msg = \"Feature names mismatch after grouping\"\n",
    "    raise ValueError(msg)\n",
    "\n",
    "# Sort by increasing importance for permutation importance plot\n",
    "sorted_perm_indices = np.argsort(sum(grouped_perm_importances[qpu] for qpu in qpus))\n",
    "sorted_perm_features = [grouped_perm_feature_names[i] for i in sorted_perm_indices]\n",
    "\n",
    "# Sort by increasing importance for feature importance plot\n",
    "sorted_indices = np.argsort(sum(grouped_feature_importances[qpu] for qpu in qpus))\n",
    "sorted_features = [grouped_feature_names[i] for i in sorted_indices]\n",
    "\n",
    "# Normalize feature importances\n",
    "for qpu in qpus:\n",
    "    grouped_feature_importances[qpu] /= grouped_feature_importances[qpu].sum()\n",
    "    grouped_perm_importances[qpu] /= grouped_perm_importances[qpu].sum()\n",
    "\n",
    "# Plot settings\n",
    "width = 0.35  # Bar width\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 5), dpi=300)\n",
    "\n",
    "# Plot feature importance\n",
    "x = np.arrange(len(sorted_features))\n",
    "for i, qpu in enumerate(qpus):\n",
    "    axs[0].barh(\n",
    "        x - width / 2 + i * width,\n",
    "        grouped_feature_importances[qpu][sorted_indices],\n",
    "        width,\n",
    "        label=names[i],\n",
    "        color=cols[i],\n",
    "        alpha=0.9,\n",
    "    )\n",
    "\n",
    "axs[0].set_yticks(x)\n",
    "axs[0].set_yticklabels(sorted_features)\n",
    "axs[0].set_xlabel(\"Feature Importance\")\n",
    "axs[0].legend(loc=\"lower right\")\n",
    "\n",
    "for spine in axs[0].spines.values():\n",
    "    spine.set_linewidth(0.25)\n",
    "axs[0].tick_params(axis=\"both\", which=\"major\", width=0.25)\n",
    "\n",
    "# Plot permutation importance\n",
    "x = np.arrange(len(sorted_perm_features))\n",
    "for i, qpu in enumerate(qpus):\n",
    "    axs[1].barh(\n",
    "        x - width / 2 + i * width,\n",
    "        grouped_perm_importances[qpu][sorted_perm_indices],\n",
    "        width,\n",
    "        label=names[i],\n",
    "        color=cols[i],\n",
    "        alpha=0.9,\n",
    "    )\n",
    "\n",
    "axs[1].set_yticks(x)\n",
    "axs[1].set_yticklabels(sorted_perm_features)\n",
    "axs[1].set_xlabel(\"Permutation Importance\")\n",
    "axs[1].legend(loc=\"lower right\")\n",
    "\n",
    "for spine in axs[1].spines.values():\n",
    "    spine.set_linewidth(0.25)\n",
    "axs[1].tick_params(axis=\"both\", which=\"major\", width=0.25)\n",
    "\n",
    "# Adjust layout and save the plot\n",
    "fig.tight_layout()\n",
    "plt.savefig(data_dir / \"figures\" / \"importance.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
